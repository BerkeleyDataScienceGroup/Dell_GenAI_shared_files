{"cells":[{"cell_type":"markdown","metadata":{"id":"ixAWYeq5Uk3V"},"source":["# Chapter 3 Exercise: Fine-Tuning a Question-Answer Model Using OpenAI\n","\n"]},{"cell_type":"markdown","metadata":{"id":"InIbVHQqP3KW"},"source":["---\n","## 1: Install Required modules:\n","\n","The modules can be tricky to load without conflicts. (Not that uncommon for rapidly changing data science software.) The commands below that force the installs of particular versions have worked as of 2/3/24. After these modules are imported, there will be a small button displayed at the end of the output for this block that reads **'RESTART SESSION'**.\n","\n","Click that button to update the notebook to these particular module versions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3Jmgz1UP7cs","tags":[]},"outputs":[],"source":["!pip install opendatasets --quiet\n","!pip install --force-reinstall typing-extensions==4.5\n","!pip install --force-reinstall openai==1.8"]},{"cell_type":"markdown","metadata":{"id":"k3pYTs11P3KY"},"source":["---\n","## 2: Import Required Libraries:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJVp4M-EP3KZ","tags":[]},"outputs":[],"source":["import pandas as pd\n","import opendatasets as od\n","from openai import OpenAI\n","from datetime import datetime\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"neWgibuiUk3X"},"source":["---\n","## 3: Load the data from Kaggle\n","\n","- Details on the [Question-Answer dataset](https://www.kaggle.com/datasets/rtatman/questionanswer-dataset) on Kaggle.\n","- To load the data from Kaggle, you'll need your Kaggle username and an API key.\n","- If you don't have a Kaggle account, you can sign up for one [here](https://www.kaggle.com/account/login?phase=startRegisterTab&returnUrl=%2F).\n","- Once you have an account, you can generate an API key on the [Settings](https://www.kaggle.com/settings) page for your account by selecting ***Create New Token***.\n","- Alternatively, you can ***download*** the data to your local machine from this page: [Question-Answer dataset](https://www.kaggle.com/datasets/rtatman/questionanswer-dataset) and then ***upload*** it to Colab.\n","\n","Our task in this notebook will be to fine-tune an existing OpenAI model using this dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJ799EVGP3KZ","tags":[]},"outputs":[],"source":["# you need your kaggle username and API key here.\n","qa_url = \"https://www.kaggle.com/datasets/rtatman/questionanswer-dataset\"\n","od.download(qa_url)"]},{"cell_type":"markdown","source":["You can see that the questionanswer-dataset has been saved locally to your colab instance.\n","\n"],"metadata":{"id":"YtShi5mGH9-A"}},{"cell_type":"code","source":["!ls"],"metadata":{"id":"HIQnSFCfUImu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls questionanswer-dataset/"],"metadata":{"id":"wCVULRvMUfE-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 4: Clean and Prepare the Dataset\n","\n","We'll start by creating three empty dataframes for each year students answered questions to produce this dataset."],"metadata":{"id":"xf17GYvHR4zI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGvZq0JS6V-q","tags":[]},"outputs":[],"source":["df_08=df_09=df_10=[]\n","print(df_08)"]},{"cell_type":"markdown","source":["Load the data and take a look at what the dataset looks like, particularly the questions and answers."],"metadata":{"id":"kVRjqZtdS1sz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qx2ecLhlP3Kb","tags":[]},"outputs":[],"source":["df_08 = pd.read_csv('questionanswer-dataset/S08_question_answer_pairs.txt', sep='\\t')\n","df_09 = pd.read_csv('questionanswer-dataset/S09_question_answer_pairs.txt', sep='\\t')\n","\n","# fix for df_10\n","df_10 = pd.read_csv('questionanswer-dataset/S10_question_answer_pairs.txt', sep='\\t', encoding = 'ISO-8859-1')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eo1_nJdZP3Kb","tags":[]},"outputs":[],"source":["df_all=pd.concat([df_08,df_09,df_10])\n","df_all.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2a9k6WVFP3Kc","tags":[]},"outputs":[],"source":["df_all.tail()"]},{"cell_type":"markdown","metadata":{"id":"55hKI4dYUk3Y"},"source":["### 4.1: Analyze the dataset:\n","\n","- How many rows and columns are there?\n","- Is there bad data inside this dataset (null values, missing etc) ?\n","- How should we deal with bad rows?\n","\n","Number of rows and columns:"]},{"cell_type":"code","source":["# your code here"],"metadata":{"id":"c0hyLlx9Wfcv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What are the fields and how complete are they?"],"metadata":{"id":"0GQYPgAIWqoK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"axt3YcmEUk3Z","tags":[]},"outputs":[],"source":["# your code here"]},{"cell_type":"markdown","metadata":{"id":"TmI-t66CP3Kd"},"source":["### 4.2: Clean up the data frame and eliminate duplicates and rows with nulls\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kP3MiWfhP3Kd","tags":[]},"outputs":[],"source":["df_qa = df_all[['Question', 'Answer']]\n","df_qa.head()"]},{"cell_type":"markdown","source":["Drop rows with ANY missing data and drop duplicate questions."],"metadata":{"id":"kx1O55rCXbvf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dl_XSDrP3Kd","tags":[]},"outputs":[],"source":["# your code here"]},{"cell_type":"markdown","source":["Check to see if we missed any NaNs"],"metadata":{"id":"LTOWshWNXl7l"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmrjiMW_P3Kd","tags":[]},"outputs":[],"source":["df_qa.isna().any()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-AX_i_yP3Kd","tags":[]},"outputs":[],"source":["df_qa.info()"]},{"cell_type":"code","source":["df_qa.head()"],"metadata":{"id":"C2QWBXSD8MIZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XN-zvR32Uk3Z"},"source":["### 4.3: Transform the cleaned dataframe into a format OpenAI uses for fine-tuning\n","\n","\n","Start by changing 'Question' and 'Answer' to 'prompt' and 'completion', respectively:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKy5G19NP3Ke","tags":[]},"outputs":[],"source":["df=df_qa.rename(columns={\"Question\": \"prompt\", \"Answer\": \"completion\"})\n","df=df.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6NqVelpxP3Ke","tags":[]},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"NIlXh6MRP3Ke"},"source":["### 4.4: Split the data into train and validation sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5PB5CfxP3Ke","tags":[]},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","# your code here, set the values of df_train and df_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDEqNpKUP3Ke","tags":[]},"outputs":[],"source":["df_train.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqGIl0UjP3Kf","tags":[]},"outputs":[],"source":["df_val.info()"]},{"cell_type":"markdown","metadata":{"id":"haFhMnkCP3Kf"},"source":["## 4.5 Convert dataset into json form"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjcW8JV1P3Kf","tags":[]},"outputs":[],"source":["df_train.to_json(\"qadatasetTrain.jsonl\", orient='records', lines=True)\n","df_val.to_json(\"qadatasetval.jsonl\", orient='records', lines=True)\n","df.to_json(\"qadataset.jsonl\", orient='records', lines=True)"]},{"cell_type":"markdown","metadata":{"id":"2BRNAKMAP3Kg"},"source":["### 4.6 Create the OpenAI client and upload your data to OpenAI\n","\n","Replace `<your key here>` with your OpenAI API key:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCWh3kZSgPyz","tags":[]},"outputs":[],"source":["api_key = \"<your key here\""]},{"cell_type":"code","source":[],"metadata":{"id":"VTfv6870Z2uu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create your OpenAI client"],"metadata":{"id":"MdzpeGuvvzQV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oqPWQlKk-m3","tags":[]},"outputs":[],"source":["# your code here"]},{"cell_type":"markdown","source":["Make sure your files are local to your colab working directory:"],"metadata":{"id":"YfI2udBWwBzn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5s9sGfHCh39z","tags":[]},"outputs":[],"source":["!ls"]},{"cell_type":"markdown","source":["Upload your datafiles to OpenAI"],"metadata":{"id":"-CsGl5CExv0m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzXuWbxZP3Kk","tags":[]},"outputs":[],"source":["file_name_ls = [\"qadataset.jsonl\",\n","               \"qadatasetval.jsonl\",\n","               \"qadatasetTrain.jsonl\"]\n","\n","upload_response={}\n","\n","for file_name in file_name_ls:\n","  upload_response[file_name] = client.files.create(\n","  file=open(file_name, \"rb\"),\n","  purpose='fine-tune')"]},{"cell_type":"markdown","source":["`upload_response` is a dictionary we create to hold the information on how these files are stored within OpenAI. The most important of these are the OpenAI 'versions' of your data files."],"metadata":{"id":"bzFsTnwoyRFm"}},{"cell_type":"code","source":["upload_response"],"metadata":{"id":"jQEH_zClyQ-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To retrieve one of these file names you'd pass it the json name as a key to the dictionary and append `.id` to access the file id:\n","\n"],"metadata":{"id":"CJPQAXwNyQtu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_9IQpk_m_qp","tags":[]},"outputs":[],"source":["upload_response['qadatasetTrain.jsonl'].id"]},{"cell_type":"markdown","source":["---\n","## 5: Fine tune OpenAI's 'base' davinci-002 model\n","\n","*OpenAI's models are changing pretty rapidly, so don't be surprised if you need to change the name of this model to a successor model if davinci-002 is deprecated. The error message will point you towards options for the successor model to use.*"],"metadata":{"id":"WsrPXJ78uzCU"}},{"cell_type":"markdown","source":["### 5.1: Kick off a fine-tuning job.\n","\n","- The `fine_tuning.jobs.create()` method of `client` is the key here.\n","- On my base version of colab, this job took 12-15 minutes to run.\n","\n","- **IF YOU CAN'T WAIT THAT LONG FOR A MODEL TO TRAIN, SKIP TO SECTION 6 of this notebook below \"6: Examine the Fine-Tuned Model's Performance\". IF YOU UNCOMMENT THE INDICATED LINE IN THAT SECTION'S FIRST CELL, YOU CAN LOAD A PREVIOUSLY TRAINED MODEL INSTEAD OF WAITING FOR YOUR NEW ONE TO RUN.**\n","\n"],"metadata":{"id":"CDtRQr1gz4VQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkqz7A-cP3Kk","tags":[]},"outputs":[],"source":["train_file_id = upload_response['qadatasetTrain.jsonl'].id\n","val_file_id = upload_response['qadatasetval.jsonl'].id\n","\n","# your code here\n","\n","dict(fine_tune_response)"]},{"cell_type":"markdown","source":["### 5.2 Checking on the status of your job\n","\n","You can use this code to check the status of your job:"],"metadata":{"id":"pc6qw3_03PvB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uM_r1tcp0vjg","tags":[]},"outputs":[],"source":["job_id=fine_tune_response.id\n","\n","print(f'job status is: {client.fine_tuning.jobs.retrieve(job_id).status} \\n')\n","\n","events = client.fine_tuning.jobs.list_events(job_id)\n","\n","ts = events.data[0].created_at\n","dt = datetime.fromtimestamp(ts)\n","\n","print(f\"Most recent event: {events.data[0].message}\")\n","print(f\"Occurred at: {dt}\")\n"]},{"cell_type":"markdown","source":["You can use this code to see each step in the status of your job, with the most recent event first and the first event listed last."],"metadata":{"id":"ZarbSBQ_e9pl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2weMp8k4P3Kk","tags":[]},"outputs":[],"source":["import signal\n","#import datetime\n","\n","def signal_handler(sig, frame):\n","  status = client.fine_tuning.jobs.retrieve(job_id).status\n","  print(f\"Stream interrupted. Job is still {status}.\")\n","  return\n","\n","print(f'Streaming events for the fine-tuning job: {job_id}\\n')\n","signal.signal(signal.SIGINT, signal_handler)\n","\n","events = client.fine_tuning.jobs.list_events(job_id)\n","\n","try:\n","  for event in events.data:\n","      print(f'{datetime.fromtimestamp(event.created_at)} {event.message}')\n","except Exception:\n","    print(\"Stream interrupted (client disconnected).\")\n"]},{"cell_type":"markdown","source":["### 5.4: Retrieving Prior Fine-Tuning Jobs\n","\n","Running fine-tuning jobs when you already have working ones (say, because you colab session timed out) can be expensive and time-consuming, especially if you're working with real, rather than educational datasets.\n","\n","There are a few ways to retrieve your prior jobs:\n","- From the [Fine-tuning](https://platform.openai.com/finetune) dashboard on OpenAi's website\n","- Programatically, which we'll show below.\n","\n","The object below will get you a list of all of your recent fine-tuning jobs. We'll comment it out because its output can get long, but you're free to uncomment it and run it."],"metadata":{"id":"BVjo7frsjig_"}},{"cell_type":"code","source":["#client.fine_tuning.jobs.list().__dict__ ['data']"],"metadata":{"id":"l9h4U0LK5ytG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you want to get the most recently completed fine-tuned job, you'd just append [0] to the above to get the first item in the list. (Or n to get the n+1th job.)"],"metadata":{"id":"Q8lcohlSAmvv"}},{"cell_type":"code","source":["ftjob_example = client.fine_tuning.jobs.list().__dict__ ['data'][0]\n","ftjob_example"],"metadata":{"id":"pmJPqMbFAle8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The key pieces of information you're likely to want to extract are:\n","- The fine-tuning job ID\n","- The name of the fine-tuned model associated with the job\n","- The result file(s), which provides performance measures at each step for the job as the model is fine-tuned\n","\n","The next three cells show the commands you'd use to retrieve each:"],"metadata":{"id":"FrCrGvPs54JG"}},{"cell_type":"code","source":["ftjob_example.id"],"metadata":{"id":"Fj124k_yChv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ftjob_example.fine_tuned_model"],"metadata":{"id":"byafw2ioChJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ftjob_example.result_files[0]"],"metadata":{"id":"gHzjuW415ydg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will provide you with some of the key pieces of information for the jobs you've run:"],"metadata":{"id":"PovjrfaFEJRa"}},{"cell_type":"code","source":["# print up to the last 5 fine tuning jobs in reverse order of completion\n","# (most recent first)\n","max_jobs = 5\n","\n","for i, ft_job in enumerate(client.fine_tuning.jobs.list().__dict__ ['data']):\n","  #'{datetime.fromtimestamp(event.created_at)\n","  if i == max_jobs:\n","    break\n","  try:\n","    print(f\"Fine-tuning job completed: {datetime.fromtimestamp(ft_job.finished_at)}\") # completed at\n","    print(f\"Fine-tuning job ID: {ft_job.id}\") # job\n","    print(f\"Fine-tuned model: {ft_job.fine_tuned_model} \\n\")\n","  except:\n","    print(\"Job hasn't completed (and thus no complete date & time)\")\n"],"metadata":{"id":"NBxypsF-4Zl-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 6: Examine the Fine-Tuned Model's Performance\n","\n","Note: *If you run this before the job you kicked-off above completes, you'll get your last completed job rather than the new job.*"],"metadata":{"id":"zRPTrD3JEtnu"}},{"cell_type":"code","source":["# get the last completed job\n","ft_job_new = client.fine_tuning.jobs.list().__dict__ ['data'][0]\n","\n","# IF YOU CAN'T WAIT FOR THE JOB TO RUN, YOU CAN LOAD AN ALREADY TRAINED MODEL BY UNCOMMENTING THE LINE BELOW\n","#ft_job_new = client.fine_tuning.jobs.retrieve('ftjob-oqjKe4BrjUfJHiINkWtDGwQw')"],"metadata":{"id":"Ufk0rYfDL-m_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get the performance results for the last job and save those results locally as a CSV file."],"metadata":{"id":"bKd_VEVrMfrH"}},{"cell_type":"code","source":[],"metadata":{"id":"L-ViQpgfRgry"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V42IMfi2Hpae"},"outputs":[],"source":["response_file = ft_job_new.result_files[0]\n","print(f\"Name of response_file: {response_file}\")\n","\n","## dynamic response file ID\n","resultsdv=client.files.content(response_file)\n","resultsdv.write_to_file(\"compiled_results.csv\")\n"]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"cJlEsg9o5aAa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KOy0bmBP3Kl"},"source":["### 6.1 Reading the Performance Results into a Dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68xqM3WGQwpg"},"outputs":[],"source":["#Evaluation Metric for Fine tuned model\n","\n","import sys\n","if sys.version_info[0] < 3:\n","    from StringIO import StringIO\n","else:\n","    from io import StringIO\n","\n","TESTDATA = StringIO(resultsdv.text)\n","df = pd.read_csv(TESTDATA, sep=\",\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4YxhUD7K9o9"},"outputs":[],"source":["df"]},{"cell_type":"markdown","source":["### 6.2: A Quick Visualistion of Model Performance\n","\n","We want to see a general decrease in loss and a general increase in accuracy."],"metadata":{"id":"R38EoKSJT4KQ"}},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","\n","axes[0].plot(df.train_loss)\n","axes[0].set_title(\"Model Training Loss\")\n","axes[0].set_xlabel(\"steps (# of batches)\")\n","axes[1].plot(df.train_accuracy)\n","axes[1].set_title(\"Model Training Accuracy\")\n","axes[1].set_xlabel(\"steps (# of batches)\")\n",";"],"metadata":{"id":"-9v1V60YT39W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ktgwYLMlP3Kn"},"source":["---\n","## 7: Evaluation Metrics for Fine-Tuned Models"]},{"cell_type":"markdown","metadata":{"id":"tVtNUbzgP3Kn"},"source":["- **elapsed_examples**: the number of examples the model has seen so far (including repeats), where one example is one element in your batch. For example, if batch_size = 4, each step will increase elapsed_examples by 4.\n","\n","\n","- **elapsed_tokens**: the number of tokens the model has seen so far (including repeats).\n","\n","- **training_loss**: loss on the training batch\n","\n","- **training_sequence_accuracy**: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67\n","\n","\n","- **training_token_accuracy**: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83\n","\n","Small batch sizes are one reason the loss can be quite variable from batch to batch even as overall loss measures improve."]},{"cell_type":"markdown","metadata":{"id":"KiogYTo_Uk3Z"},"source":["---\n","## 8: Compare the Fine-Tuned Models' Answers to the Base Models'\n","\n","When are the fine-tuned models answer's superior to the base model's answers and vice-versa?"]},{"cell_type":"code","source":["# the model you fine-tuned above:\n","new_model = ft_job_new.fine_tuned_model\n","print(new_model)\n"],"metadata":{"id":"cbG7NKzhv6VQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The prompt (Question) and completion (Answer) from the model you fine-tuned."],"metadata":{"id":"an1_Ob6vwcVF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J75VI0biP3Ko","tags":[]},"outputs":[],"source":["prompt =\"When did Lincoln begin his political career? ->\"\n","\n","result = client.completions.create(model=new_model, prompt=prompt,\n","                                  max_tokens=30, temperature=0,\n","                                  top_p=1, n=1, stop=['.','\\n'])\n","result.choices[0].text\n"]},{"cell_type":"markdown","source":["The same for the 'base' model that is not fine-tuned:"],"metadata":{"id":"LsFAm-eEwrrU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yzZEZsqP3Kp","tags":[]},"outputs":[],"source":["prompt =\"When did Lincoln begin his political career?\\nAnswer\"\n","#prompt =\"When did Lincoln begin his political career? ->\"\n","base_model = \"davinci-002\"\n","\n","result = client.completions.create(model=base_model, prompt=prompt,\n","                                  max_tokens=30, temperature=0,\n","                                  top_p=1, n=1, stop=['.','\\n'])\n","\n","result.choices[0].text"]},{"cell_type":"markdown","source":["Write a function that makes comparing the results between the base and new model a little more concise.\n","\n","- It should take new_model, type and your prompt and print:\n","  - The the prompt\n","  - The name of the new model\n","  - The response\n","\n","  and the same for the base model:"],"metadata":{"id":"6a4ghRLSxmTM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMZQfpKaZmvj","tags":[]},"outputs":[],"source":["### STUDENT CODE BELOW\n","\n","def newbasecompare(new_model, base_model, prompt):\n","# your code here\n"]},{"cell_type":"markdown","source":["We'll use our function to look at few more examples."],"metadata":{"id":"f2Aq77640pgK"}},{"cell_type":"code","source":["prompt =\"Who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds?\"\n","base_model = \"davinci-002\"\n","newbasecompare(new_model, base_model, prompt)"],"metadata":{"id":"IcZrsvmRys3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5X83hlXP3Kp","tags":[]},"outputs":[],"source":["prompt=\"What do beetles eat?\"\n","base_model = \"davinci-002\"\n","newbasecompare(new_model, base_model, prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPeg1oYrbZbu","tags":[]},"outputs":[],"source":["prompt=\"What are the similarities between beetles and grasshoppers?\"\n","base_model = \"davinci-002\"\n","newbasecompare(new_model, base_model, prompt)"]},{"cell_type":"markdown","source":["Lastly, we'll swtich the base model from davinci-002 to gpt-3.5-turbo-instruct, which is more advanced (and quite a bit more expensive)."],"metadata":{"id":"pqGY9U1TzgwQ"}},{"cell_type":"code","source":["prompt =\"Who was the first to perform and publish careful experiments aiming at the definition of an international temperature scale on scientific grounds?\"\n","base_model = \"gpt-3.5-turbo-instruct\"\n","newbasecompare(new_model, base_model, prompt)"],"metadata":{"id":"-XZstX2ozelg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt=\"What do beetles eat?\"\n","base_model = \"gpt-3.5-turbo-instruct\"\n","newbasecompare(new_model, base_model, prompt)"],"metadata":{"id":"ipuev1IozeOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt=\"What are the similarities between beetles and grasshoppers?\"\n","base_model = \"gpt-3.5-turbo-instruct\"\n","newbasecompare(new_model, base_model, prompt)"],"metadata":{"id":"YJGHJ0TBzdyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt=\"When did Lincoln begin his political career?\"\n","base_model = \"gpt-3.5-turbo-instruct\"\n","newbasecompare(new_model, base_model, prompt)"],"metadata":{"id":"OCz5DHpNDVUq"},"execution_count":null,"outputs":[]}],"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96},{"_defaultOrder":57,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.trn1.2xlarge","vcpuNum":8},{"_defaultOrder":58,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1.32xlarge","vcpuNum":128},{"_defaultOrder":59,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.trn1n.32xlarge","vcpuNum":128}],"colab":{"provenance":[]},"instance_type":"ml.m5.large","kernelspec":{"display_name":"Python 3 (Data Science 3.0)","language":"python","name":"python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}